{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ffc273-985f-4ff2-a718-98f9b52d8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/onnx/tensorflow-onnx.git\n",
      "  Cloning https://github.com/onnx/tensorflow-onnx.git to /tmp/pip-req-build-uf5s83ym\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/onnx/tensorflow-onnx.git /tmp/pip-req-build-uf5s83ym\n",
      "  Resolved https://github.com/onnx/tensorflow-onnx.git to commit 4fed7de9534b6a084f7f2326bae775545bd97f9e\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.1 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from tf2onnx==1.16.1) (2.3.5)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from tf2onnx==1.16.1) (1.17.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from tf2onnx==1.16.1) (2.32.3)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from tf2onnx==1.16.1) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from tf2onnx==1.16.1) (25.9.23)\n",
      "Requirement already satisfied: protobuf~=3.20 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from tf2onnx==1.16.1) (3.20.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from requests->tf2onnx==1.16.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from requests->tf2onnx==1.16.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from requests->tf2onnx==1.16.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from requests->tf2onnx==1.16.1) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# normally you install it with pip like that:\n",
    "# pip install tf2onnx\n",
    "# but the last release (Jan 2025) doesn't contain numpy 2 update\n",
    "# so install it from github directly:\n",
    "\n",
    "!pip install git+https://github.com/onnx/tensorflow-onnx.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6d2e3e-e504-490c-bc5b-b4170bf7584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-10 07:51:03--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx\n",
      "Resolving github.com (github.com)... 4.237.22.38\n",
      "Connecting to github.com (github.com)|4.237.22.38|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T08%3A41%3A46Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T07%3A41%3A16Z&ske=2025-12-10T08%3A41%3A46Z&sks=b&skv=2018-11-09&sig=iclv9tDMh5EMDUtYtFEUfreCKGy4OjTPPNK%2FMDY%2BTBY%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTM1MzM2MywibmJmIjoxNzY1MzUzMDYzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.ecjkMN9b-k5moUXh1Xh2ZpZR9erzEOe2XaqMlPtqthA&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-12-10 07:51:03--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T08%3A41%3A46Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T07%3A41%3A16Z&ske=2025-12-10T08%3A41%3A46Z&sks=b&skv=2018-11-09&sig=iclv9tDMh5EMDUtYtFEUfreCKGy4OjTPPNK%2FMDY%2BTBY%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTM1MzM2MywibmJmIjoxNzY1MzUzMDYzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.ecjkMN9b-k5moUXh1Xh2ZpZR9erzEOe2XaqMlPtqthA&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10337 (10K) [application/octet-stream]\n",
      "Saving to: ‘hair_classifier_v1.onnx.2’\n",
      "\n",
      "hair_classifier_v1. 100%[===================>]  10.09K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-12-10 07:51:03 (111 MB/s) - ‘hair_classifier_v1.onnx.2’ saved [10337/10337]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266bd4f8-a99a-4d8a-9880-560a5da5d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-10 07:51:27--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data\n",
      "Resolving github.com (github.com)... 4.237.22.38\n",
      "Connecting to github.com (github.com)|4.237.22.38|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T08%3A36%3A07Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T07%3A35%3A37Z&ske=2025-12-10T08%3A36%3A07Z&sks=b&skv=2018-11-09&sig=sesUNEQQVr176hzk7MNsJg96RWyred8u%2Fuua5Bqkk80%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTM1NDg4NywibmJmIjoxNzY1MzUzMDg3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.dTyaGlrYgkKUhtsnFwVylk3Aj0eFNe64B-XJctROw-I&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-12-10 07:51:28--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T08%3A36%3A07Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T07%3A35%3A37Z&ske=2025-12-10T08%3A36%3A07Z&sks=b&skv=2018-11-09&sig=sesUNEQQVr176hzk7MNsJg96RWyred8u%2Fuua5Bqkk80%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTM1NDg4NywibmJmIjoxNzY1MzUzMDg3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.dTyaGlrYgkKUhtsnFwVylk3Aj0eFNe64B-XJctROw-I&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 80355328 (77M) [application/octet-stream]\n",
      "Saving to: ‘hair_classifier_v1.onnx.data.2’\n",
      "\n",
      "hair_classifier_v1. 100%[===================>]  76.63M  34.7MB/s    in 2.2s    \n",
      "\n",
      "2025-12-10 07:51:30 (34.7 MB/s) - ‘hair_classifier_v1.onnx.data.2’ saved [80355328/80355328]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ceb0d8-47c4-4fc0-a0b8-b22f47775a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW9.ipynb                  hair_classifier_v1.onnx.data\n",
      "hair_classifier_v1.onnx    hair_classifier_v1.onnx.data.1\n",
      "hair_classifier_v1.onnx.1  hair_classifier_v1.onnx.data.2\n",
      "hair_classifier_v1.onnx.2  \u001b[0m\u001b[01;35myf_dokzqy3vcritme8ggnzqlvwa.jpeg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "109bf3af-0747-4622-8a16-9fdf4da55392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the output layer is: output\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the model you just downloaded\n",
    "model = onnx.load(\"hair_classifier_v1.onnx\")\n",
    "\n",
    "# Access the model graph\n",
    "graph = model.graph\n",
    "\n",
    "# The output layer information is stored in 'output'\n",
    "output_node = graph.output[0]\n",
    "\n",
    "print(f\"The name of the output layer is: {output_node.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525f740d-76af-4432-9be0-3a5cd2be9204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /home/ubuntu/anaconda3/lib/python3.12/site-packages (12.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cce15e7-7e2f-48e5-9d71-f0761a01d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e348cd51-ebec-4a10-9da7-74a1e127b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-10 07:52:26--  https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\n",
      "Resolving habrastorage.org (habrastorage.org)... 95.47.173.34, 95.47.173.35, 2a14:b680:0:56::34, ...\n",
      "Connecting to habrastorage.org (habrastorage.org)|95.47.173.34|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 398272 (389K) [image/jpeg]\n",
      "Saving to: ‘yf_dokzqy3vcritme8ggnzqlvwa.jpeg.1’\n",
      "\n",
      "yf_dokzqy3vcritme8g 100%[===================>] 388.94K   293KB/s    in 1.3s    \n",
      "\n",
      "2025-12-10 07:52:31 (293 KB/s) - ‘yf_dokzqy3vcritme8ggnzqlvwa.jpeg.1’ saved [398272/398272]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f6a8a5-b133-400f-a6b0-07afdac68a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-image-helper in /home/ubuntu/anaconda3/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: numpy>=2.3.2 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from keras-image-helper) (2.3.5)\n",
      "Requirement already satisfied: pillow>=11.3.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from keras-image-helper) (12.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-image-helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674ed8ce-475b-41c6-9e08-1bd4982f6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in /home/ubuntu/anaconda3/lib/python3.12/site-packages (1.23.2)\n",
      "Requirement already satisfied: coloredlogs in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from onnxruntime) (25.9.23)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from onnxruntime) (2.3.5)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from onnxruntime) (24.1)\n",
      "Requirement already satisfied: protobuf in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from onnxruntime) (3.20.3)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from onnxruntime) (1.13.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/anaconda3/lib/python3.12/site-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96b98ef0-3513-4209-b3bc-b2e725971144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Results for yf_dokzqy3vcritme8ggnzqlvwa.jpeg ---\n",
      "Class 0: 100.00%\n",
      "\n",
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Converts raw model output (logits) into confidence percentages.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1)\n",
    "\n",
    "# 1. Initialize the Session\n",
    "onnx_model_path = \"hair_classifier_v1.onnx\"\n",
    "session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    # Open, convert to RGB, and force load to prevent '_im' buffer errors\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img.load()\n",
    "    \n",
    "    # Resize to exact model specs: 200x200\n",
    "    img = img.resize((200, 200), Image.BILINEAR)\n",
    "    \n",
    "    # Convert to array and normalize to [0, 1]\n",
    "    img_array = np.array(img).astype(np.float32) / 255.0\n",
    "    \n",
    "    # MODEL REQUIREMENT: Transpose from (H, W, C) to (C, H, W)\n",
    "    # This turns (200, 200, 3) into (3, 200, 200)\n",
    "    img_array = img_array.transpose(2, 0, 1)\n",
    "    \n",
    "    # Add batch dimension: (1, 3, 200, 200)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "\n",
    "\n",
    "# 2. Run the Prediction\n",
    "try:\n",
    "    test_image = 'yf_dokzqy3vcritme8ggnzqlvwa.jpeg' # Ensure this file exists in your directory\n",
    "    X = prepare_image(test_image)\n",
    "    \n",
    "    # Run session\n",
    "    raw_preds = session.run([output_name], {input_name: X})[0]\n",
    "    \n",
    "    # Apply Softmax to get probabilities\n",
    "    probabilities = softmax(raw_preds)[0]\n",
    "    \n",
    "    # 3. Output results\n",
    "    print(f\"--- Results for {test_image} ---\")\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        print(f\"Class {i}: {prob*100:.2f}%\")\n",
    "        \n",
    "    print(f\"\\nPredicted Class: {np.argmax(probabilities)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{test_image}' was not found. Please upload it to your environment.\")\n",
    "except Exception as e:\n",
    "    print(f\"Inference Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aee039d0-451b-4c03-9e85-767a18f8f5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First Pixel R-Channel Analysis ---\n",
      "Original R-value at (0, 0): 57\n",
      "Estimated Normalized R-value: 0.223529\n",
      "Final Preprocessed R-value (X[0, 0, 0, 0]): 0.243137\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ... (Assume softmax and prepare_image functions are defined here as in the previous response) ...\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Converts raw model output (logits) into confidence percentages.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1)\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    # Open, convert to RGB, and force load to prevent '_im' buffer errors\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img.load()\n",
    "    \n",
    "    # Force copy and resize to 200x200\n",
    "    img_data = np.array(img.copy()) \n",
    "    img_resized = Image.fromarray(img_data).resize((200, 200), Image.BILINEAR)\n",
    "    \n",
    "    # Convert to array, normalize to [0, 1]\n",
    "    img_array = np.array(img_resized).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Transpose to NCHW\n",
    "    img_array = img_array.transpose(2, 0, 1)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# --- Verification Code ---\n",
    "try:\n",
    "    test_image = 'yf_dokzqy3vcritme8ggnzqlvwa.jpeg' # Ensure this file exists\n",
    "    X = prepare_image(test_image)\n",
    "    \n",
    "    # Check the value at the R channel (index 0) of the first pixel (0, 0)\n",
    "    # The format is [Batch, Channel, Height, Width]\n",
    "    r_channel_value = X[0, 0, 0, 0]\n",
    "    \n",
    "    # 1. Get the original pixel value for comparison (0-255)\n",
    "    original_img = Image.open(test_image).convert('RGB')\n",
    "    original_pixel = original_img.getpixel((0, 0)) # Gets R, G, B at (0, 0)\n",
    "    \n",
    "    # 2. Estimate the expected normalized value (since resizing changes the exact value)\n",
    "    estimated_normalized = original_pixel[0] / 255.0\n",
    "    \n",
    "    print(f\"--- First Pixel R-Channel Analysis ---\")\n",
    "    print(f\"Original R-value at (0, 0): {original_pixel[0]}\")\n",
    "    print(f\"Estimated Normalized R-value: {estimated_normalized:.6f}\")\n",
    "    print(f\"Final Preprocessed R-value (X[0, 0, 0, 0]): {r_channel_value:.6f}\")\n",
    "    \n",
    "    # Run the inference to ensure the array X is valid for the model\n",
    "    # (Optional, run only if you have the session initialized)\n",
    "    # session = ort.InferenceSession(\"hair_classifier_v1.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "    # raw_preds = session.run([output_name], {input_name: X})[0]\n",
    "    # ...\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{test_image}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during verification: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fb1afa7-fc95-42d0-853e-f1e3db9d2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First Pixel R-Channel Verification ---\n",
      "Preprocessed R-value (X[0, 0, 0, 0]): 0.243137\n",
      "\n",
      "--- Model Output (Raw Logits) ---\n",
      "Raw Output Array: [[2.0972447]]\n",
      "Predicted Class Index (Max Logit): 0\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# We define a dummy softmax function just to keep the structure clean, but it is not called for the prediction.\n",
    "def softmax(x):\n",
    "    # This function is not used in the output, but kept for context.\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1)\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    # Open, convert to RGB, and force load to prevent '_im' buffer errors\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img.load()\n",
    "    \n",
    "    # Force copy and resize to 200x200\n",
    "    img_data = np.array(img.copy()) \n",
    "    img_resized = Image.fromarray(img_data).resize((200, 200), Image.BILINEAR)\n",
    "    \n",
    "    # Convert to array, normalize to [0, 1]\n",
    "    img_array = np.array(img_resized).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Transpose from (H, W, C) to (C, H, W)\n",
    "    img_array = img_array.transpose(2, 0, 1)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# 1. Initialize the Session\n",
    "onnx_model_path = \"hair_classifier_v1.onnx\"\n",
    "session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "# 2. Run the Prediction and Verification\n",
    "try:\n",
    "    test_image = 'yf_dokzqy3vcritme8ggnzqlvwa.jpeg' # Ensure this file exists\n",
    "    X = prepare_image(test_image)\n",
    "    \n",
    "    # --- R-Channel Verification ---\n",
    "    # The format is [Batch, Channel, Height, Width]\n",
    "    r_channel_value = X[0, 0, 0, 0]\n",
    "    \n",
    "    print(f\"--- First Pixel R-Channel Verification ---\")\n",
    "    print(f\"Preprocessed R-value (X[0, 0, 0, 0]): {r_channel_value:.6f}\")\n",
    "    \n",
    "    # --- Prediction (Raw Logits) ---\n",
    "    raw_preds = session.run([output_name], {input_name: X})[0]\n",
    "    \n",
    "    # Logits are the raw numbers; they can be positive, negative, or large.\n",
    "    print(f\"\\n--- Model Output (Raw Logits) ---\")\n",
    "    print(f\"Raw Output Array: {raw_preds}\")\n",
    "    print(f\"Predicted Class Index (Max Logit): {np.argmax(raw_preds)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{test_image}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Inference Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "691301b6-17ae-4264-b0d5-adeeaff739d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/ml-zoomcamp-sandy'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92833bd6-6966-4bca-aa75-e31aa7bfed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from keras_image_helper import create_preprocessor\n",
    "\n",
    "preprocessor = create_preprocessor(\"xception\", target_size=(200, 200))\n",
    "\n",
    "session = ort.InferenceSession(\n",
    "    \"hair_classifier_v1.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "classes = [\n",
    "   \"Wavy Hair\",\n",
    "   \"Straight Hair\",\n",
    "   \"Curly Hair\",\n",
    "   \"Kinky/Coily Hair\",\n",
    "]\n",
    "\n",
    "\n",
    "def predict(url):\n",
    "    X = preprocessor.from_url(url)\n",
    "    X = X.transpose(0, 3, 1, 2)  # Swaps axes: (Batch, H, W, C) -> (Batch, C, H, W)\n",
    "    print(f\"Shape sent to ONNX: {X.shape}\")\n",
    "    result = session.run([output_name], {input_name: X})\n",
    "    float_predictions = result[0][0].tolist()\n",
    "    return dict(zip(classes, float_predictions))\n",
    "\n",
    "\n",
    "#def lambda_handler(event, context):\n",
    "#    url = event[\"url\"]\n",
    "#    result = predict(url)\n",
    "#    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefcaa10-3523-4403-84c4-3bdbb9230f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile\t\t\thair_classifier_v1.onnx.data.2\n",
      "HW9.ipynb\t\t\tlamda_deploy.py\n",
      "hair_classifier_v1.onnx\t\tmodel.bin\n",
      "hair_classifier_v1.onnx.1\tpyproject.toml\n",
      "hair_classifier_v1.onnx.2\ttrain.py\n",
      "hair_classifier_v1.onnx.3\tuv.lock\n",
      "hair_classifier_v1.onnx.data\tyf_dokzqy3vcritme8ggnzqlvwa.jpeg\n",
      "hair_classifier_v1.onnx.data.1\tyf_dokzqy3vcritme8ggnzqlvwa.jpeg.1\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee5b7c7-d29b-4822-9a34-87c092069a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/ml-zoomcamp-sandy/machine-learning-zoomcamp-homework/09-Serverless'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60992286-9d3a-4505-9abc-2d6fe21d386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape sent to ONNX: (1, 3, 200, 200)\n",
      "--- Prediction Successful ---\n",
      "{'Wavy Hair': -0.20132379233837128}\n"
     ]
    }
   ],
   "source": [
    "test_url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\" # REPLACE THIS\n",
    "prediction_scores = predict(test_url)\n",
    "print(\"--- Prediction Successful ---\")\n",
    "print(prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a488c6-30eb-41f7-aa28-10246a9b1e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
